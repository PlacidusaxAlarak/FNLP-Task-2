# preprocess.py (æœ€ç»ˆæ­£ç¡®ç‰ˆ - é€‚ç”¨äºæ— è¡¨å¤´æ•°æ®é›†)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import os

def load_data_and_split(file_path, test_size=0.1):
    """åŠ è½½æ•°æ®å¹¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    try:
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ˜ç¡®æŒ‡å®šæ–‡ä»¶æ²¡æœ‰è¡¨å¤´ï¼Œå¹¶ç›´æ¥ä¸ºåˆ—å‘½å
        df = pd.read_csv(file_path, sep='\t', header=None, names=['sentence', 'label'])
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        print("è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„ã€ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„ä¸¤åˆ—æ–‡ä»¶ã€‚")
        return None, None, None, None

    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹
    df['label'] = df['label'].astype(int)
    sentences = df['sentence'].fillna('').apply(lambda x: str(x).lower().split()).tolist()
    labels = df['label'].tolist()

    train_texts, val_texts, train_labels, val_labels = train_test_split(
        sentences, labels, test_size=test_size, random_state=42, stratify=labels
    )
    return train_texts, val_texts, train_labels, val_labels


def load_test_data(filepath):
    """åŠ è½½æµ‹è¯•æ•°æ® (é€»è¾‘ä¸ä¸Šé¢ä¿æŒä¸€è‡´)"""
    try:
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šåŒæ ·åœ°ï¼Œæ²¡æœ‰è¡¨å¤´ï¼Œç›´æ¥å‘½å
        df = pd.read_csv(filepath, sep='\t', header=None, names=['sentence', 'label'])
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
        return None, None

    df['label'] = df['label'].astype(int)
    sentences = df['sentence'].fillna('').apply(lambda x: str(x).lower().split()).tolist()
    labels = df['label'].tolist()
    return sentences, labels


def build_glove_embedding_matrix(glove_path, vocab, embedding_dim):
    """æ„å»ºGloVeè¯å‘é‡çŸ©é˜µ"""
    print("æ­£åœ¨åŠ è½½GloVeè¯å‘é‡...")
    embeddings_index = {}
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="è¯»å–GloVeæ–‡ä»¶"):
            values = line.split()
            word = values[0]
            if len(values) == embedding_dim + 1:
                coefs = np.asarray(values[1:], dtype='float32')
                embeddings_index[word] = coefs

    embedding_matrix = np.zeros((len(vocab), embedding_dim))
    found_words = 0
    for word, i in vocab.word2idx.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            found_words += 1
        else:
            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))

    print(f"åœ¨GloVeè¯æ±‡è¡¨ä¸­æ‰¾åˆ°äº† {found_words}/{len(vocab)} ä¸ªå•è¯ã€‚")
    return embedding_matrix